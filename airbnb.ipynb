{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNffiM8Y49Mtou0YosQz6iD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirakshareddy/airbnb/blob/main/airbnb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUYFUdNgAYuK"
      },
      "outputs": [],
      "source": [
        "###Neural Network\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "\n",
        "# Read the LA_Lising.csv\n",
        "df = pd.read_csv('LA_Listings.csv', encoding='ISO-8859-1')\n",
        "df.head()\n",
        "# print(\"hii\")\n",
        "\n",
        "# Read the NY_Listings.csv\n",
        "df2 = pd.read_csv('NY_Listings.csv', encoding='ISO-8859-1')\n",
        "df2.head()\n",
        "\n",
        "# Read the airbnb_ratings_new.csv\n",
        "df3 = pd.read_csv('airbnb_ratings_new.csv', encoding='ISO-8859-1')\n",
        "pd.set_option('display.max_columns', None)\n",
        "df3.head()\n",
        "\n",
        "filter_df = df3[df3['Country'] == 'United States']\n",
        "\n",
        "filter_df.head()\n",
        "\n",
        "df_combined = df.append(df2)\n",
        "df_final = df_combined.append(filter_df)\n",
        "excel_file = pd.ExcelWriter('Final Dataset.xlsx')\n",
        "df_final.to_excel(excel_file, index=False)\n",
        "\n",
        "excel_file.save()\n",
        "\n",
        "df_final = pd.read_excel('/content/drive/MyDrive/Pattern Recognization/Final Dataset.xlsx')\n",
        "\n",
        "df_final.describe()\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "#Scikit-learn for implemeting LinearRegression from a existing algorithm.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# To plot pretty figures\n",
        "# %matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def cost_compute(X, y, theta):\n",
        "    return 1/(2*y.size)*np.sum(np.square(X.dot(theta)-y))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_final, test = train_test_split(df_final, test_size=0.3, random_state=43)\n",
        "\n",
        "# Drop any rows with null value\n",
        "df_final.dropna(axis=0, how='any', inplace=True)\n",
        "test.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "column_names = ['Host total listings count', 'longitude', 'Accommodates', 'Bathrooms', 'Bedrooms', 'Minimum nights', 'Maximum nights', 'Availability 365', 'Number of reviews',\n",
        "                'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication', 'Review Scores Location', 'Review Scores Value', 'Reviews per month']\n",
        "X_train = df_final[column_names]\n",
        "y_train = df_final['Price']\n",
        "\n",
        "X_test = test[column_names]\n",
        "y_test = test['Price']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "input_train_scaled = scaler.fit_transform(X_train)\n",
        "output_train_scaled = scaler.fit_transform(y_train.values.reshape(-1,1))\n",
        "input_test_scaled = scaler.fit_transform(X_test)\n",
        "output_test_scaled = scaler.fit_transform(y_test.values.reshape(-1,1))\n",
        "\n",
        "input_train_scaled.shape\n",
        "\n",
        "output_test_scaled.shape\n",
        "\n",
        "def activation_relu(z): # takes a numpy array as input and returns activated array\n",
        "    relu = np.maximum(0,z)\n",
        "    return relu\n",
        "\n",
        "def parameter_initialization(size_layer): #takes a list of the layer sizes as input and returns initialized parameters\n",
        "    param = {}\n",
        "    for i in range(1, len(size_layer)):\n",
        "        param['W' + str(i)] = np.random.randn(size_layer[i], size_layer[i-1])*0.01\n",
        "        param['B' + str(i)] = np.random.randn(size_layer[i],1)*0.01\n",
        "    return param\n",
        "\n",
        "def forward_pass(X_train, param): #takes input training features and parameters as input and returns a dictionary containining the numpy arrays of activations of all layer\n",
        "    layer = len(param)//2\n",
        "    value = {}\n",
        "    for i in range(1, layer+1):\n",
        "        if i==1:\n",
        "            value['Z' + str(i)] = np.dot(param['W' + str(i)], X_train) + param['B' + str(i)]\n",
        "            value['A' + str(i)] = activation_relu(value['Z' + str(i)])\n",
        "        else:\n",
        "            value['Z' + str(i)] = np.dot(param['W' + str(i)], value['A' + str(i-1)]) + param['B' + str(i)]\n",
        "            if i==layer:\n",
        "                value['A' + str(i)] = value['Z' + str(i)]\n",
        "            else:\n",
        "                value['A' + str(i)] = activation_relu(value['Z' + str(i)])\n",
        "    return value\n",
        "\n",
        "def compute_cost(value, Y_train): #takes true value and dictionary having activations of all layer as input and returns cost\n",
        "    layer = len(value)//2\n",
        "    Y_pred = value['A' + str(layer)]\n",
        "    cost = 1/(2*len(Y_train)) * np.sum(np.square(Y_pred - Y_train))\n",
        "    return cost\n",
        "\n",
        "def back_propagation(param, value, X_train, Y_train): #takes parameters, activations, training set as input and returns gradients wrt parameters\n",
        "    layer = len(param)//2\n",
        "    m = len(Y_train)\n",
        "    grads = {}\n",
        "    for i in range(layer,0,-1):\n",
        "        if i==layer:\n",
        "            dA = 1/m * (value['A' + str(i)] - Y_train)\n",
        "            dZ = dA\n",
        "        else:\n",
        "            dA = np.dot(param['W' + str(i+1)].T, dZ)\n",
        "            dZ = np.multiply(dA, np.where(value['A' + str(i)]>=0, 1, 0))\n",
        "        if i==1:\n",
        "            grads['W' + str(i)] = 1/m * np.dot(dZ, X_train.T)\n",
        "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        else:\n",
        "            grads['W' + str(i)] = 1/m * np.dot(dZ,value['A' + str(i-1)].T)\n",
        "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    return grads\n",
        "\n",
        "def param_update(param, grads, lr): #takes parameters, gradients and learning rate as input and returns updated parameters\n",
        "    layer = len(param)//2\n",
        "    params_updated = {}\n",
        "    for i in range(1,layer+1):\n",
        "        params_updated['W' + str(i)] = param['W' + str(i)] - lr * grads['W' + str(i)]\n",
        "        params_updated['B' + str(i)] = param['B' + str(i)] - lr * grads['B' + str(i)]\n",
        "    return params_updated\n",
        "\n",
        "def NN(X_train, Y_train, size_layer, n_iters, lr): #trains the NN\n",
        "    param = parameter_initialization(size_layer)\n",
        "    for i in range(n_iters):\n",
        "        value = forward_pass(X_train.T, param)\n",
        "        cost = compute_cost(value, Y_train.T)\n",
        "        grads = back_propagation(param, value,X_train.T, Y_train.T)\n",
        "        param = param_update(param, grads, lr)\n",
        "        print('Cost at iteration ' + str(i+1) + ' = ' + str(cost) + '\\n')\n",
        "    return param\n",
        "\n",
        "def accuracy(X_train, X_test, Y_train, Y_test, param): #compute accuracy on test and training data given learnt parameters\n",
        "    values_train = forward_pass(X_train.T, param)\n",
        "    values_test = forward_pass(X_test.T, param)\n",
        "    train_acc = np.sqrt(mean_squared_error(Y_train, values_train['A' + str(len(size_layer)-1)].T))\n",
        "    test_acc = np.sqrt(mean_squared_error(Y_test, values_test['A' + str(len(size_layer)-1)].T))\n",
        "    return train_acc, test_acc\n",
        "\n",
        "def predict(X, param):  #predict on new array X given learnt parameters\n",
        "    value = forward_pass(X.T, param)\n",
        "    predictions = value['A' + str(len(value)//2)].T\n",
        "    return predictions\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def activation_relu(z):\n",
        "    a = np.maximum(0,z)\n",
        "    return a\n",
        "\n",
        "def parameter_initialization(size_layer):\n",
        "    param = {}\n",
        "    for i in range(1, len(size_layer)):\n",
        "        param['W' + str(i)] = np.random.randn(size_layer[i], size_layer[i-1])*0.01\n",
        "        param['B' + str(i)] = np.random.randn(size_layer[i],1)*0.01\n",
        "    return param\n",
        "\n",
        "def forward_pass(X_train, param):\n",
        "    layer = len(param)//2\n",
        "    value = {}\n",
        "    for i in range(1, layer+1):\n",
        "        if i==1:\n",
        "            value['Z' + str(i)] = np.dot(param['W' + str(i)], X_train) + param['B' + str(i)]\n",
        "            value['A' + str(i)] = activation_relu(value['Z' + str(i)])\n",
        "        else:\n",
        "            value['Z' + str(i)] = np.dot(param['W' + str(i)], value['A' + str(i-1)]) + param['B' + str(i)]\n",
        "            if i==layer:\n",
        "                value['A' + str(i)] = value['Z' + str(i)]\n",
        "            else:\n",
        "                value['A' + str(i)] = activation_relu(value['Z' + str(i)])\n",
        "    return value\n",
        "\n",
        "def compute_cost(value, Y_train):\n",
        "    layer = len(value)//2\n",
        "    Y_pred = value['A' + str(layer)]\n",
        "    cost = 1/(2*len(Y_train)) * np.sum(np.square(Y_pred - Y_train))\n",
        "    return cost\n",
        "\n",
        "def back_propagation(param, value, X_train, Y_train):\n",
        "    layer = len(param)//2\n",
        "    m = len(Y_train)\n",
        "    grads = {}\n",
        "    for i in range(layer,0,-1):\n",
        "        if i==layer:\n",
        "            dA = 1/m * (value['A' + str(i)] - Y_train)\n",
        "            dZ = dA\n",
        "        else:\n",
        "            dA = np.dot(param['W' + str(i+1)].T, dZ)\n",
        "            dZ = np.multiply(dA, np.where(value['A' + str(i)]>=0, 1, 0))\n",
        "        if i==1:\n",
        "            grads['W' + str(i)] = 1/m * np.dot(dZ, X_train.T)\n",
        "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        else:\n",
        "            grads['W' + str(i)] = 1/m * np.dot(dZ,value['A' + str(i-1)].T)\n",
        "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    return grads\n",
        "\n",
        "def param_update(param, grads, lr):\n",
        "    layer = len(param)//2\n",
        "    params_updated = {}\n",
        "    for i in range(1,layer+1):\n",
        "        params_updated['W' + str(i)] = param['W' + str(i)] - lr * grads['W' + str(i)]\n",
        "        params_updated['B' + str(i)] = param['B' + str(i)] - lr * grads['B' + str(i)]\n",
        "    return params_updated\n",
        "\n",
        "def NN(X_train, Y_train, size_layer, n_iters, lr):\n",
        "    param = parameter_initialization(size_layer)\n",
        "    for i in range(n_iters):\n",
        "        value = forward_pass(X_train.T, param)\n",
        "        cost = compute_cost(value, Y_train.T)\n",
        "        grads = back_propagation(param, value,X_train.T, Y_train.T)\n",
        "        param = param_update(param, grads, lr)\n",
        "        print('Cost at iteration ' + str(i+1) + ' = ' + str(cost) + '\\n')\n",
        "    return param\n",
        "\n",
        "def accuracy(X_train, X_test, Y_train, Y_test, param):\n",
        "    values_train = forward_pass(X_train.T, param)\n",
        "    values_test = forward_pass(X_test.T, param)\n",
        "    train_acc = np.sqrt(mean_squared_error(Y_train, values_train['A' + str(len(size_layer)-1)].T))\n",
        "    test_acc = np.sqrt(mean_squared_error(Y_test, values_test['A' + str(len(size_layer)-1)].T))\n",
        "    return train_acc, test_acc\n",
        "\n",
        "def predict(X, param):\n",
        "    value = forward_pass(X.T, param)\n",
        "    predictions = value['A' + str(len(value)//2)].T\n",
        "    return predictions\n",
        "\n",
        "size_layer = [16, 5, 5, 1]                                                       #set layer sizes, do not change the size of the first and last layer \n",
        "n_iters = 200                                                                  #set number of iterations over the training set(also known as epochs in batch gradient descent context)\n",
        "lr = 0.000001                                                              #set learning rate for gradient descent\n",
        "param = NN(input_train_scaled, output_train_scaled, size_layer, n_iters, lr)           #train the NN\n",
        "train_acc, test_acc = accuracy(input_train_scaled, input_test_scaled, output_train_scaled, output_test_scaled, param)  #get training and test accuracy\n",
        "print('RMSE Training Data = ' + str(train_acc))\n",
        "print('RMSE Test Data = ' + str(test_acc))\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "regr = MLPRegressor(random_state=1, max_iter=500).fit(input_train_scaled, output_train_scaled)\n",
        "\n",
        "regr.score(input_test_scaled, output_test_scaled)\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "DT = DecisionTreeRegressor(random_state = 16, criterion = 'mse', max_depth = None, splitter = 'best')\n",
        "DT.fit(input_train_scaled, output_train_scaled)\n",
        "\n",
        "pred = DT.predict(input_test_scaled)\n",
        "r2_score(output_test_scaled, pred)\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "regressor=SVR(kernel='rbf',gamma='auto')\n",
        "\n",
        "regressor.fit(input_train_scaled, output_train_scaled)\n",
        "test_pre = regressor.predict(input_test_scaled)\n",
        "train_pre = regressor.predict(input_train_scaled)\n",
        "\n",
        "prediction = regressor.predict(input_test_scaled)\n",
        "mse = mean_squared_error(output_test_scaled, prediction)\n",
        "rmse = mse**.5\n",
        "r2 = r2_score(output_test_scaled, prediction)\n",
        "print(mse)\n",
        "print(rmse)\n",
        "print(r2)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 16).fit(input_train_scaled, output_train_scaled)\n",
        "\n",
        "prediction = rf.predict(input_test_scaled)\n",
        "mse = mean_squared_error(output_test_scaled, prediction)\n",
        "rmse = mse**.5\n",
        "r2 = r2_score(output_test_scaled, prediction)\n",
        "print(mse)\n",
        "print(rmse)\n",
        "print(r2)\n",
        "\n"
      ]
    }
  ]
}